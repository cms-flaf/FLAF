{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FLAF","text":"<p>FLAF - Flexible LAW-based Analysis Framework. Task workflow managed is done via LAW (Luigi Analysis Framework).</p>"},{"location":"#how-to-install","title":"How to install","text":"<ol> <li> <p>Setup ssh keys:</p> <ul> <li>On GitHub settings/keys</li> <li>On CERN GitLab profile/keys</li> </ul> </li> <li> <p>Clone the repository:   <pre><code>git clone --recursive git@github.com:cms-flaf/Framework.git FLAF\n</code></pre></p> </li> <li> <p>Create a user customisation file <code>config/user_custom.yaml</code>. It should contain all user-specific modifications that you don't want to be committed to the central repository. Below is example of minimal content of the file (replace <code>USER_NAME</code> and <code>ANA_FOLDER</code> with your values):     <pre><code>fs_default:\n    - 'T3_CH_CERNBOX:/store/user/USER_NAME/ANA_FOLDER/'\nfs_anaCache:\n    - 'T3_CH_CERNBOX:/store/user/USER_NAME/ANA_FOLDER/'\nfs_anaTuple:\n    - 'T3_CH_CERNBOX:/store/user/USER_NAME/ANA_FOLDER/'\nfs_anaCacheTuple:\n    - 'T3_CH_CERNBOX:/store/user/USER_NAME/ANA_FOLDER/'\nfs_histograms:\n- 'T3_CH_CERNBOX:/store/user/USER_NAME/ANA_FOLDER/histograms/'\nfs_json:\n- 'T3_CH_CERNBOX:/store/user/USER_NAME/ANA_FOLDER/jsonFiles/'\nanalysis_config_area: config/HH_bbtautau\ncompute_unc_variations: true\nstore_noncentral: true\n</code></pre></p> </li> </ol>"},{"location":"#how-to-load-environment","title":"How to load environment","text":"<ol> <li> <p>Following command activates the framework environment:     <pre><code>source env.sh\n</code></pre></p> </li> <li> <p>For the new installation or after you implement new law tasks, you need to update the law index:     <pre><code>law index --verbose\n</code></pre></p> </li> <li> <p>Initialize voms proxy:     <pre><code>voms-proxy-init -voms cms -rfc -valid 192:00\n</code></pre></p> </li> </ol>"},{"location":"analysis/","title":"Common analysis steps","text":"<p>Remarks:</p> <ul> <li> <p>commands bellow assume that <code>ERA</code> variable is set. E.g.     <pre><code>ERA=Run2_2016\n</code></pre>     Alternatively you can add <code>ERA=Run2_2016; ...</code> in front of each command.</p> </li> <li> <p><code>version</code> argument alows to produce different versions of the same task. In the command below <code>--version dev</code> is used for illustration purposes. You can replace it with your version naming.</p> </li> <li><code>--workflow</code> can be <code>htcondor</code> or <code>local</code>. It is recommended to develop and test locally and then switch to <code>htcondor</code> for production. In examples below <code>--workflow local</code> is used for illustration purposes.</li> <li>when running on <code>htcondor</code> it is recommended to add <code>--transfer-logs</code> to the command to transfer logs to local.</li> <li><code>--customisations</code> argument is used to pass custom parameters to the task in form param1=value1,param2=value2,...     IMPORTANT for HHbbTauTau analysis: if running using deepTau 2p5 add <code>--customisations deepTauVersion=2p5</code></li> <li>if you want to run only on few files, you can specify list of branches to run using <code>--branches</code> argument. E.g. <code>--branches 2,7-10,17</code>.</li> <li>to get status, use <code>--print-stauts N,K</code> where N is depth for task dependencies, K is depths for file dependencies. E.g. <code>--print-status 3,1</code>.</li> <li>to remove task output use <code>--remove-output N,a</code>, where N is depth for task dependencies. E.g. <code>--remove-output 0,a</code>.</li> <li>it is highly recommended to limitate the maximum number of parallel jobs running adding <code>--parallel-jobs M</code> where M is the number of the parallel jobs (e.g. M=100)</li> </ul>"},{"location":"analysis/#create-input-file-list","title":"Create input file list","text":"<pre><code>law run InputFileTask  --period ${ERA} --version dev\n</code></pre>"},{"location":"analysis/#create-anacache","title":"Create anaCache","text":"<pre><code>law run AnaCacheTask  --period ${ERA} --version dev\n</code></pre>"},{"location":"analysis/#create-anatuple","title":"Create anaTuple","text":"<pre><code>law run AnaTupleTask --period ${ERA} --version dev\n</code></pre>"},{"location":"analysis/#merge-data","title":"Merge data","text":"<pre><code>law run DataMergeTask --period ${ERA} --version dev\n</code></pre> <ul> <li>note: It's very important to first run <code>InputFileTask</code> then the other tasks dependencies are automatically fixed (e.g. if running <code>AnaTupleTask</code> without <code>AnaCacheTask</code>, it will first run <code>AnaCacheTask</code> then <code>AnaTupleTask</code>). If you do not run <code>InputFileTask</code>, running other tasks will raise error.</li> </ul>"},{"location":"hh_bbtautau/","title":"HH-&gt;bb\\(\\tau\\)\\(\\tau\\) analysis steps","text":"<p>Commands below assume that AnaTuples have already been produced. If not, please produce them following the instruction in the analysis section.</p> <p>Remember that:</p> <ul> <li><code>ERA</code> variable is set. E.g.     <pre><code>ERA=Run2_2016\n</code></pre>     Alternatively you can add <code>ERA=Run2_2016; ...</code> in front of each command.     Run2 possible eras are: <code>Run2_2016</code>,<code>Run2_2016_HIPM</code>,<code>Run2_2017</code> and <code>Run2_2018</code> </li> <li>when expliciting <code>VERSION_NAME</code> variable, its name contains explicitly the deepTau version: <code>VERSION_NAME= vXX_deepTauYY_ZZZ</code>, where:<ul> <li>XX is the anaTuple version (if not the first production it can be useful to have <code>v1,v2,..</code>),</li> <li>YY is the deepTau version (<code>2p1</code> or <code>2p5</code>)</li> <li>ZZZ are other eventual addition (e.g. if only tauTau channel <code>_onlyTauTau</code> or if <code>Zmumu</code> ntuples <code>_Zmumu</code>..) </li> </ul> </li> <li><code>--workflow</code> can be <code>htcondor</code> or <code>local</code>. It is recommended to develop and test locally and then switch to <code>htcondor</code> for production. In examples below <code>--workflow local</code> is used for illustration purposes. </li> <li>when running on <code>htcondor</code> it is recommended to add <code>--transfer-logs</code> to the command to transfer logs to local. </li> <li><code>--customisations</code> argument is used to pass custom parameters to the task in form param1=value1,param2=value2,...     IMPORTANT for HHbbTauTau analysis: if running using deepTau 2p5 add <code>--customisations deepTauVersion=2p5</code> </li> <li>if you want to run only on few files, you can specify list of branches to run using <code>--branches</code> argument. E.g. <code>--branches 2,7-10,17</code>. </li> <li>to get status, use <code>--print-stauts N,K</code> where N is depth for task dependencies, K is depths for file dependencies. E.g. <code>--print-status 3,1</code>. </li> <li>to remove task output use <code>--remove-output N,a</code>, where N is depth for task dependencies. E.g. <code>--remove-output 0,a</code>. </li> <li>it is highly recommended to limitate the maximum number of parallel jobs running adding <code>--parallel-jobs M</code> where M is the number of the parallel jobs (e.g. M=100)</li> </ul>"},{"location":"hh_bbtautau/#create-anacachetuple","title":"Create anaCacheTuple","text":"<p>For each Anatuple, an anaCacheTuple (storing observables which are computationally heavier) will be created.</p> <p><pre><code>law run AnaCacheTupleTask --period ${ERA} --version ${VERSION_NAME}\n</code></pre> Note: at the <code>AnaCacheTupleTask</code> stage, the addition of customisation for specifying the version is still needed. For the other tasks, it won't be needed anymore.</p>"},{"location":"hh_bbtautau/#merge-data-in-anacache-tuples","title":"Merge data in anaCache tuples","text":"<pre><code>law run DataCacheMergeTask --period ${ERA} --version ${VERSION_NAME}\n</code></pre>"},{"location":"hh_bbtautau/#histograms-production","title":"Histograms Production","text":"<p>This has to be run after AnaTupleTask but not necessairly after AnaCacheTupleTask, if the variable to plot is not stored inside AnaCacheTuples.</p> <p>These task will produce histograms with observables that need to be specified inside the <code>Analysis/tasks.py</code> file, specifically inside the <code>vars_to_plot</code> list.</p> <p>The tasks to run are the following:</p> <ol> <li><code>HistProducerFileTask</code>: for each AnaTuple an histogram of the corresponding variable will be created.     <pre><code>law run HistProducerFileTask --period $ERA --version ${VERSION_NAME}\n</code></pre></li> <li><code>HistProducerSampleTask</code>: all the histogram belonging to a specific sample will be merged in one histogram.     <pre><code>law run HistProducerSampleTask --period $ERA --version ${VERSION_NAME}\n</code></pre></li> <li> <p><code>MergeTask</code>: all the histogram will be merged from samples to only one histograms under the folder <code>${HISTOGRAMS}/all_histograms/</code> to a specific sample will be merged in one histogram. At this stage, for each norm/shape uncertainty (+ central scenario) will be created one histogram.     <pre><code>law run MergeTask --period $ERA --version ${VERSION_NAME}\n</code></pre>     Each histograms will be named as: <code>all_histograms_UNCERTAINTY.root</code> where uncertainty can be [Central, TauES_DM0, ecc....]</p> </li> <li> <p><code>HaddMergedTask</code>: all the merged histograms (produced separately for each uncertainty) will be merged in only one file.     <pre><code>law run HaddMergedTask --period $ERA --version ${VERSION_NAME}\n</code></pre>     Tip: It's very fast so it can be convenient to run this task in local.     The final histogram will be named as: <code>all_histograms_Hadded.root</code></p> </li> </ol>"},{"location":"hh_bbtautau/#how-to-run-hhbtag-training-skim-ntuple-production","title":"How to run HHbtag training skim ntuple production","text":"<pre><code>python Studies/HHBTag/CreateTrainingSkim.py --inFile $CENTRAL_STORAGE/prod_v1/nanoAOD/2018/GluGluToBulkGravitonToHHTo2B2Tau_M-350.root --outFile output/skim.root --mass 350 --sample GluGluToBulkGraviton --year 2018 &gt;&amp; EventInfo.txt\npython Common/SaveHisto.txt --inFile $CENTRAL_STORAGE/prod_v1/nanoAOD/2018/GluGluToBulkGravitonToHHTo2B2Tau_M-350.root --outFile output/skim.root\n</code></pre>"},{"location":"stat_inference/","title":"Statistical inference","text":""},{"location":"stat_inference/#how-to-run-limits","title":"How to run limits","text":"<ol> <li> <p>As a temporary workaround, if you want to run multiplie commands, to avoid delays to load environment each time run:   <pre><code>cmbEnv /bin/zsh # or /bin/bash\n</code></pre>   Alternatively add <code>cmbEnv</code> in front of each command. E.g.   <pre><code>cmbEnv python3 -c 'print(\"hello\")'\n</code></pre></p> </li> <li> <p>Create datacards.   <pre><code>python3 StatInference/dc_make/create_datacards.py --input PATH_TO_SHAPES  --output PATH_TO_CARDS --config PATH_TO_CONFIG\n</code></pre>   Available configurations:</p> <ul> <li>For X-&gt;HH&gt;bbtautau Run 2: StatInference/config/x_hh_bbtautau_run2.yaml</li> <li>For X-&gt;HH-&gt;bbWW Run 3: StatInference/config/x_hh_bbww_run3.yaml</li> </ul> </li> <li> <p>Run limits.   <pre><code>law run PlotResonantLimits --version dev --datacards 'PATH_TO_CARDS/*.txt' --xsec fb --y-log\n</code></pre>   Hints:</p> <ul> <li>use <code>--workflow htcondor</code> to submit on HTCondor (by default it runs locally)</li> <li>add <code>--remove-output 4,a,y</code> to remove previous output files</li> <li>add <code>--print-status 0</code> to get status of the workflow (where <code>0</code> is a depth). Useful to get the output file name.</li> <li>for more details see cms-hh inference documentation</li> </ul> </li> <li> <p>Plot Pulls and Impacts   <pre><code>PlotPullsAndImpacts --version dev --datacards \"PATH_TO_CARDS/specific_card.txt\"  --hh-model NO_STR --parameter-values r=1 --parameter-ranges r,-100,100 --method robust --PlotPullsAndImpacts-order-by-impact True --mc-stats True --PullsAndImpacts-custom-args=\"--expectSignal=1\"\n</code></pre>   Hints:</p> <ul> <li>Don't use datacards as *.txt because pulls should be done for each mass point separately</li> <li>add <code>--remove-output 4,a,y</code> to remove previous output files</li> <li>add <code>--print-status 0</code> to get status of the workflow (where <code>0</code> is a depth). Useful to get the output file name.</li> </ul> </li> </ol>"}]}